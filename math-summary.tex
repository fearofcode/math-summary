\documentclass{article}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{amsthm}
\makeatletter
\def\th@plain{%
  \thm@notefont{}% same as heading font
  \itshape % body font
}
\def\th@definition{%
  \thm@notefont{}% same as heading font
  \normalfont % body font
}
\makeatother

\begin{document}

\title{Math review for machine learning}
\author{Warren Henning\\\texttt{warren.henning@gmail.com}}
\date{\today}

\maketitle

\section{Disclaimer}

I haven't taken a university math course in several years. There may very well
be typos and outright mathematical errors!

Strictly speaking, I have no business writing this.

\section{Introduction}

This document is intended to summarize the essential math needed to understand
machine learning at a deep level. It is intended to summarize key formulas and
definitions, not replace actual texts.

\section{Calculus}

The following is taken from Spivak's {\it Calculus} \cite{spivak}.

\subsection{Limits}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

\begin{definition}[$(\epsilon, \delta)$ definition of limit]

Let $f: \mathbb{R} \to \mathbb{R}$ be a function. 

\begin{equation}
\lim_{x \to a} f(x) = L
\end{equation}

if for every $\epsilon > 0$ there exists $\delta > 0$ such that if $0 < |x-a| <
\delta$, then $|f(x) - L| < \epsilon$.
 
\end{definition}

This formalizes the idea of being able to take $f(x)$ as close to $L$ as we want
by making $x$ sufficiently close to $a$.

\subsection{Continuity}

\begin{definition}

$f$ is continuous at $a$ if

\begin{equation}
\lim_{x \to a} f(x) = f(a).
\end{equation}

\end{definition}

\subsection{Derivatives}

\begin{definition} 
$f$ is differentiable at $a$ if

\begin{equation}
\lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\end{equation}

exists.
\end{definition}

Recall the geometric intuition of tangent lines; $f'(x)$ is like a secant line
on a function between two points of infinitely small distance. The physical
interpretation of $f'(x)$ as velocity and $f''(x)$ as acceleration should be familiar.

\begin{theorem}[Product Rule] 

If $f$ and $g$ are differentiable, 

\begin{equation}
(f \cdot g)' = f'g + g'f.
\end{equation}


\end{theorem} 

This generalizes recursively; $(f \cdot g \cdot h)' = f'gh + fg'h + fgh'$, and
so on. Notice the symbolic symmetry of it: sum the product of all the functions,
differentiating each one in turn.

\begin{theorem}[Chain Rule]

If $f$ and $g$ are differentiable,

\begin{equation}
(f \circ g)'(x) = f'(g)g'(x).
\end{equation}


\end{theorem}

\subsubsection{Derivative formulas}

\begin{equation}
\left(\frac{f}{g}\right)' = \frac{f'g - g'f}{g^2}
\end{equation}

\begin{equation}
(x^n)' = nx^{n-1}
\end{equation}

\subsection{Integration}

\subsection{Taylor Series}

\section{Linear Algebra}

\section{Analysis}

\section{Probability}

\section{Statistics}

\section{Conclusion}

The benefits of machine learning justify the considerable mathematical
background involved because it allows you to do in, say, 100 lines of code what
would not otherwise be possible in 1,000 or 10,000 of conventional procedural
programming logic. It lets you create systems that learn, adapt, and adjust to
changing conditions. They offer some of the greatest expressive power per line
of code possible on a computer.

Go forth and kick ass!

\section{Books}

\begin{thebibliography}{9}

\bibitem{spivak}
  Michael Spivak,
  \emph{Calculus}.
  Publish or Perish,
  4th Edition,
  2008.

\end{thebibliography}

\section{License}

This document is freely shareable under the terms of the GNU Free Documentation
license; see LICENSE in this document's directory for more information.

\end{document}

