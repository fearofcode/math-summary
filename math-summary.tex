\documentclass{article}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{amsthm}
\makeatletter
\def\th@plain{%
  \thm@notefont{}% same as heading font
  \itshape % body font
}
\def\th@definition{%
  \thm@notefont{}% same as heading font
  \normalfont % body font
}
\makeatother

\begin{document}

\title{Math review for machine learning}
\author{Warren Henning\\\texttt{warren.henning@gmail.com}}
\date{\today}

\maketitle

\section{Disclaimer}

I haven't taken a university math course in several years. There may very well
be typos and outright mathematical errors!

Strictly speaking, I have no business writing this.

\section{Introduction}

This document is intended to summarize the essential mathematical background
needed to understand machine learning at a deep level. It is intended to
summarize key ideas, not replace actual texts.

Our goal is to establish a rigorous foundation for understanding machine
learning and statistics at about a first-year graduate level by understanding
measure-theoretic probability and linear algebra.

More important than any particular formula or mathematical fact is
\emph{mathematical thinking}. There are more than enough data mining and machine learning
tools and libraries out there already; the shortage is of capable people who
can apply these tools in sound ways to add value to the organization on whose
behalf they act. You'll have to be focused on getting things done and managing
your time efficiently, as well.

If you never took an upper-division math course or a class on proof techniques,
I suspect things will get hard when you start getting into analysis. In addition
to mastering the mathematical material, you'll want to focus on rigorous
mathematical logic.

However, don't let this discourage you. I am a buffoon who has no business
attempting anything involving machine learning, and well, here I am, attempting it.

\section{Calculus}

The following is taken from Spivak's {\it Calculus} \cite{spivak}.

\subsection{Limits}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

\begin{definition}[$(\epsilon, \delta)$ definition of limit]

Let $f: \mathbb{R} \to \mathbb{R}$ be a function. 

\begin{equation}
\lim_{x \to a} f(x) = L
\end{equation}

if for every $\epsilon > 0$ there exists $\delta > 0$ such that if $0 < |x-a| <
\delta$, then $|f(x) - L| < \epsilon$.
 
\end{definition}

This formalizes the idea of being able to take $f(x)$ as close to $L$ as we want
by making $x$ sufficiently close to $a$.

Actually proving these winds up being an exercise in grappling with bullshit
absolute value expressions. Let $\epsilon > 0$ and find a suitable expression
for $\delta$ in terms of $\epsilon$ so that $|f(x) -L| < \epsilon$ holds.

\begin{example} 

\emph{Show that $\lim_{x \to 1} (3x-1) = 2$}.

Substituting $f(x) = 3x-1$, $a = 1$, and $L = 2$ into the definition of a limit, we
want to show that 

\begin{equation}
|x - 1| < \delta \implies |3x - 3| < \epsilon.
\end{equation}

This example chosen because the algebra is easy works out like so: $|3x-3| =
|3(x-1)| = 3|x-1| < \epsilon \implies |x-1| < \epsilon/3$, so $\delta =
\epsilon/3$ is a suitable choice.

Then $|x - 1| < \epsilon/3 \implies |3x-3| = |(x - 1) + (2x-2)| \leq |x-1| +
|2x-2| = |x-1| + 2|x-1| < \epsilon/3 + 2 \cdot \epsilon/3 = \epsilon,$ so we are
done. $\blacksquare$

Notice that we have used the fact that $|x + y| \leq |x| + |y|$.

\end{example}

As you can tell, with harder examples this shit gets really depressing.

\subsection{Continuity}

\begin{definition}

$f$ is continuous at $a$ if

\begin{equation}
\lim_{x \to a} f(x) = f(a).
\end{equation}

\end{definition}

\subsection{Derivatives}

\begin{definition} 
$f$ is differentiable at $a$ if

\begin{equation}
\lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\end{equation}

exists.
\end{definition}

Recall the geometric intuition of tangent lines; $f'(x)$ is like a secant line
on a function between two points of infinitely small distance. The physical
interpretation of $f'(x)$ as velocity and $f''(x)$ as acceleration should be familiar.

\begin{theorem}[Product Rule] 

If $f$ and $g$ are differentiable, 

\begin{equation}
(f \cdot g)' = f'g + g'f.
\end{equation}


\end{theorem} 

This generalizes recursively; $(f \cdot g \cdot h)' = f'gh + fg'h + fgh'$, and
so on. Notice the symbolic symmetry of it: sum the product of all the functions,
differentiating each one in turn.

\begin{theorem}[Chain Rule]

If $f$ and $g$ are differentiable,

\begin{equation}
(f \circ g)'(x) = f'(g)g'(x).
\end{equation}


\end{theorem}

\subsubsection{Derivative formulas}

\begin{equation}
\left(\frac{f}{g}\right)' = \frac{f'g - g'f}{g^2}
\end{equation}

\begin{equation}
(x^n)' = nx^{n-1}
\end{equation}

\subsection{Integration}

\subsection{Taylor Series}

\section{Linear Algebra}

\section{Analysis}

\section{Probability}

\section{Statistics}

\section{Conclusion}

The benefits of machine learning justify the considerable mathematical
background involved because it allows you to do in, say, 100 lines of code what
would not otherwise be possible in 1,000 or 10,000 of conventional procedural
programming logic. It lets you create systems that learn, adapt, and adjust to
changing conditions. They offer some of the greatest expressive power per line
of code possible on a computer.

Go forth and kick ass!

\section{Books}

\begin{thebibliography}{9}

\bibitem{spivak}
  Michael Spivak,
  \emph{Calculus}.
  Publish or Perish,
  4th Edition,
  2008.

\end{thebibliography}

\section{License}

This document is freely shareable under the terms of the GNU Free Documentation
license; see LICENSE in this document's directory for more information.

\end{document}

