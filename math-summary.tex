\documentclass{article}

\usepackage{epigraph}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=blue,
    linkcolor=blue,
    urlcolor=blue
}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{axiom}{Axiom}[section]
\newtheorem{corollary}{Corollary}[section]

\makeatletter
\def\th@plain{%
  \thm@notefont{}% same as heading font
  \itshape % body font
}
\def\th@definition{%
  \thm@notefont{}% same as heading font
  \normalfont % body font
}
\makeatother

\begin{document}

\title{Math summary for actually understanding machine learning}
\author{Warren Henning\\\texttt{warren.henning@gmail.com}}
\date{\today}

\maketitle

\tableofcontents

\pagebreak

\epigraph{Do not ask permission to understand. Do not wait for the word of authority. Seize reason in your own hand. With your own teeth savor the fruit.}{Strichartz, \cite{strichartz}}

\epigraph{We shall not cease from exploration. And the end of all our exploring will be to arrive where we started and know the place for the first time.}{T. S. Eliot}

\epigraph{Dimidium facti qui coepit habet: sapere aude (``He who has begun is half done: dare to know").}{Horace}

\epigraph{What we hope ever to do with ease, we must first learn to do with diligence.}{Samuel Johnson}

\epigraph{Get your fundamentals on lock so that you can start getting into the ill advanced shit. This is universally applicable.}{Earl Sweatshirt}

\section{Disclaimer}

I haven't taken a university math course in many years. There may very well
be typos and outright mathematical errors!

Strictly speaking, I have no business writing this.

\section{Introduction}

This document is intended to do two things: first, to summarize key theorems of difficult mathematical subjects as I'm studying them for my own purposes. Eventually, it would be nice to have this be a summary of the math needed for machine learning. But for now, things will be even more terse than an upper-division textbook.

I want to actually understand what's going on in machine learning. That means having a solid background in core pure and applied mathematics as well as probability and statistics. Following cookbook recipes for machine learning libraries is wonderful up until it doesn't work or produces something that doesn't make sense. Then you're fucked because you don't know what's going on. Googling for what could be happening yields weird stuff that makes your eyes glaze over, and then you realize you're both in way over your head and \emph{proper fucked}.

So the goal is to avoid that by having enough background to know what's going on numerically and statistically in a rigorous way. That will hopefully let us choose the right tool to begin with: do we actually need a neural network at all? Can we get away with a simple linear model? Why or why not? Can we even do any better than a simple naive Bayes classifier? The tutorials and ``data science" material won't answer these questions. They spend so much time ``visualizing" data and then writing Medium.com blog posts about a cool D3.js trick they found while doing so that they apparently don't stop to think whether what they're doing actually makes sense from a mathematical perspective.

Also, I want to apply this stuff to actual interesting problems, not just better ways to push ads onto users' screens.

Any attempts to water things down is a recipe for fallacious thinking, not that the tech industry has a problem embracing illogical things if it suits their selfish motives.

Not everything here is actually necessary to understand machine learning. It's just stuff I learned on my way there. It's a general goal I have to cultivate mathematical maturity, and pretty much anything in math is fair game for developing that.

Interspersed between the main definitions and theorems are comments that themselves contain claims which generally require rigorous proof and are not trivial by any means. The proofs of those claims are omitted here because this document is not intended to be a substitute for the many excellent textbooks that are available. See Section \ref{bibliography} for the books I used to create this. Maybe it would be better to separate them out as propositions in of themselves the way I do with other facts below, but I've tried to focus on just the essentials. Again, this is not a textbook, and I'm assuming the material here is already known.

\section{Basic Stuff}

\subsection{Triangle Inequality}
Let $|x|$ be the absolute value of $x$.

\begin{equation} \label{triangle-inequality}
|a + b| \leq |a| + |b| 
\end{equation}

for all $a, b \in \mathbb{R}$. Geometrically, it has the interpretation that the shortest path between any two points is a straight line.

\subsection{Sets}

The following comes from Smith's \textit{Introductory Mathematics: Algebra and Analysis} \cite{smith}, which is a lovely book completely worth the exorbitant Springer purchase price for the jokes interspersed between mathematical content alone.

Let $A$, $B$, and $C$ be sets.

\begin{theorem}[Distributive law of intersection over union]
$\\ A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$.
\end{theorem}

\begin{theorem}[Distributive law of union over intersection] \label{distributive-law-2}
$\\ A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$.
\end{theorem}

These are analogous to distributive laws of fields such as $\mathbb{Q}$ and $\mathbb{R}$.

The standard, direct way to prove two sets $A$ and $B$ are equal is to take $x \in A$ and show that it's in $B$, establishing that $A \subset B$\footnote{Note that $\subset$ is used here to mean ``subset" (i.e., subset or equal), not ``strict subset." We could alternatively use $\subseteq$, but that's a whole two extra characters in \LaTeX.}, and then conversely show that $x \in B$ implies $x \in A$, showing that $B \subset A$. Then $A = B$ by definition. Here's an example of this to prove Theorem \ref{distributive-law-2}:

\begin{proof}
Let $x \in A \cup (B \cap C)$. Then $x \in A$ or $x \in B \cap C$. If $x \in A$, $x \in A \cup B$ and $x \in A \cup C$, so $x \in (A \cup B) \cap (A \cup C)$. If $x \notin A$, $x \in B \cap C$, so $x \in A \cup B$ and $x \in A \cup C$, so $x \in (A \cup B) \cap (A \cup C)$. So $A \cup (B \cap C) \subset (A \cup B) \cap (A \cup C)$.

Now suppose $x \in (A \cup B) \cap (A \cup C)$. This implies $x \in A$, so $x \in A \cup (B \cap C)$. So $(A \cup B) \cap (A \cup C) \subset A \cup (B \cap C)$.

These two facts together suffice to show that the two sets are equal, so we are done.
\end{proof} 

\begin{definition}
Let $A$ and $U$ (called the \textbf{universe}) be sets. Suppose $A \subset U$. Then $A' = U \setminus A$ is the \textbf{complement} of $A$.
\end{definition}

\begin{theorem}[De Morgan's Laws]
Let $A$ and $B$ be subsets of some fixed set.

\begin{align}
(A \cup B)' = A' \cap B' \\
(A \cap B)' = A' \cup B'
\end{align}
\end{theorem}

These are easy to see with a Venn diagram or by comparing to the logical version of them you can find on Wikipedia. I would draw one here but doing that in \LaTeX \ is a pain in the ass. Sorry, just draw it on paper or Google it if you're curious.

\subsection{Maps}

Let $f: A \rightarrow B$ be a mapping from sets $A$ to $B$.

\begin{definition}
$A$ is the \textbf{domain} and $B$ the \textbf{codomain}. $f(X) = \{f(x) \ | \ x \in A\} \subset B$ is the \textbf{range} of $f$, also called the \textbf{image} of $f$.
\end{definition}

\begin{definition}
$f$ is \textbf{surjective} or \textbf{onto} if $\forall b \in B$, $\exists a \in A$ s.t. $f(a) = b$. $f$ is a \textbf{surjection}.
\end{definition}

\begin{proposition}
If the codomain of $f$ is equal to the range, $f$ is surjective.
\end{proposition}

\begin{definition}
$f$ is \textbf{injective} or \textbf{1-1} if $a \neq a' \implies f(a) \neq f(a')$, $a, a' \in A$. $f$ is an \textbf{injection}.
\end{definition}

Taking the contrapositive of the previous definition, if $f$ is 1-1 and $f(a) = f(a')$, then $a = a'$. So you can show $f$ is injective by assuming $f(a) = f(b)$ for some $a$, $b$ $\in A$ and showing that $a = b$.

\begin{definition}
If $f$ is both injective and surjective, $f$ is \textbf{bijective} or a \textbf{bijection}.
\end{definition}

\begin{definition}
$f$ is \textbf{invertible} if there exists $g: Y \rightarrow B$ such that 

\begin{center}
$y = f(x) \iff x = g(y)$ for all $x \in X$, $y \in Y$.
\end{center}

\end{definition}

\begin{proposition}
Being bijective is a necessary and sufficient condition for being invertible. If it is invertible, its inverse is unique.
\end{proposition}

\begin{definition}
Two sets have the same cardinality if there is a bijection between them.
\end{definition}

\begin{proposition}
A composition $g \ \circ \ f : A \rightarrow C  = g(f(x)) $ of two injective maps $f : A \rightarrow B$, $g: B \rightarrow C$ is injective. \\
A composition of two surjective maps is surjective. \\
A composition of two bijective maps is bijective.
\end{proposition}

\subsection{Relations, equivalence relations, partitions}

Let $S$ be a set and $s, t, u \in S$ throughout this section.

\begin{definition}
A \textbf{relation} $\bowtie$ on $S$ is a subset of $S \times S$. Write $s \bowtie t$ if $(s, t) \in \ \bowtie$.
\end{definition}

\begin{definition}
$\bowtie$ is \textbf{reflexive} if $s \bowtie s$ $\forall s \in S$. \\
$\bowtie$ is \textbf{symmetric} if $s \bowtie t \implies t \bowtie s$. \\
$\bowtie$ is \textbf{transitive} if $s \bowtie t$ and $t \bowtie u \implies s \bowtie u$. \\
$\bowtie$ is an \textbf{equivalence relation} if is reflexive, symmetric, and transitive.
\end{definition}

Of course the most familiar example of an equivalence relation is $=$. But consider also the set of lines in the plane and the relation $||$, where $l_1 \ || \ l_2$ means $l_1$ is parallel to $l_2$. Then $||$ is an equivalence relation. Likewise with the congruence operator $\cong$ on triangles in the plane.

\begin{definition}
A \textbf{partition} of $S$ is a collection of non-empty, mutually disjoint subsets whose union is $S$.
\end{definition}

To show that a collection of sets $\{ C_1, \ldots, C_n \}$ is a partition of $S$ for $n \ge 1$:

\begin{itemize}
\item Show that $C_i \neq \emptyset$ $\forall i$
\item Show that $C_i \cap C_j = \emptyset$ $\forall \ i \neq j$
\item Show that $x \in S \implies x \in C_i$ for some $i$
\end{itemize}

An example would be partitioning the plane with lines (possibly an infinite number).

Equivalence relations also partition the set into \textbf{equivalence classes}:

\begin{definition}
Let $\sim$ be an equivalence relation on $S$ and $x \in S$. \\
$[x] = \{ y \ | \ y \in S, x \sim y\}$ is the \textbf{equivalence class} of $x$.
\end{definition}

\begin{proposition}
$\{[x] \ | \ x \in S \}$ is a partition of $S$.
\end{proposition}

\section{Calculus and Analysis}

The following is taken from various calculus/analysis books, including Spivak's {\it Calculus} \cite{spivak} and Ross' {\it Elementary Analysis} \cite{ross}. Examples often correspond to exercise in the texts and the order follows that of the books. Although I drew this from multiple books, the amount of overlap between them is pretty high, although clearly analysis books will focus more on theory and less on calculation than a calculus book.

\subsection{Bounds, sup, inf, the Completeness Axiom}

Let $S \subset \mathbb{R}$, $S \neq \emptyset, m, M \in \mathbb{R}$. 

\begin{definition}
If $s_0 \in S$ and $s_0 \ge s$ $\forall s \in S$, write $\max S = s_0$. $s_0$ is the \textbf{maximum} of $S$.
\end{definition}

\begin{definition}
If $s_0 \in S$ and $s_0 \le s$ $\forall s \in S$, write $\min S = s_0$. $s_0$ is the \textbf{minimum} of $S$.
\end{definition}

\begin{definition}
If $s \leq M$ $\forall s \in S$, $M$ is an \textbf{upper bound} of $S$. S is \textbf{bounded above}.
\end{definition}

\begin{definition}
If $m \leq s$ $\forall s \in S$, $m$ is a \textbf{lower bound} of $S$. S is \textbf{bounded below}.
\end{definition}

\begin{definition}
If $S$ is both bounded above and bounded below, $S$ is \textbf{bounded}. So $\exists$ $m, M$ s.t. $S \subset [m, M]$.
\end{definition}

\begin{definition}
If $M$ is an upper bound and $M \leq B$ for all other upper bounds $B$, $M$ is a \textbf{least upper bound} or \textbf{supremum} of $S$. Write sup $S = M$.
\end{definition}

\begin{definition}
If $m$ is a lower bound and $m \geq b$ for all other lower bounds $b$, $m$ is a \textbf{greatest lower bound} or \textbf{infimum} of $S$. Write inf $S = m$.
\end{definition}

Imagine having a bunch of points on a number line and moving a pencil from left to right until it hit upon a point on the line. That would be an infimum. Going from the opposite direct, right to left, would give a supremum. Note that the infimum and supremum do not need to be in $S$.

\begin{proposition}
If a set $S$ has a least upper bound, it is unique.
\end{proposition}

\begin{proof}
Suppose there are two least upper bounds of $S$, call them $r$ and $s$. It's easy to show that $r = s$, and therefore there's only one supremum. We can just apply the definition of what a least upper bound is here: as $s$ is an upper bound and $r$ is a least upper bound, $r \leq s$, by definition of $r$ being a least upper bound. Likewise as $r$ is an upper bound and $s$ is a least upper bound, $s \leq r$. (Again, this is just applying the fact that each is a least upper bound, so it's $\leq$ all upper bounds, including, in particular, one another.) So as $r \leq s$ and $s \leq r$, $r = s$.
\end{proof}

\begin{axiom}[Completeness Axiom, a.k.a. Least Upper Bound Property]
Every nonempty subset $S$ of $\mathbb{R}$ that is bounded above has a least upper bound $\sup S \in\mathbb{R}$.
\end{axiom}

\begin{corollary}
Every nonempty subset $S$ of $\mathbb{R}$ that is bounded below has a greatest lower bound $\inf S \in\mathbb{R}$.
\end{corollary}

\begin{proof}
Since $S$ is bounded below, $\exists$ $m \in \mathbb{R}$ s.t. $m \leq s$, $s \in S$.

Let $-S = \{ -s : s \in S \}$. Then $-m \ge -s$, so $-S$ is bounded above. So $\sup -S$ exists.

It can be shown that $\inf S = - \sup -S$, so $\inf S$ exists.

\end{proof}


\begin{example} \label{supexample}
Let $a \in \mathbb{R}$. Let $S = \{r \in \mathbb{R} : 0 \leq r < a\}$. Then we claim the obvious fact that $\sup S = a$, which we will now try to prove somewhat carefully in a pattern reminiscent of other proofs of other facts that are not as entirely obvious. First, observe that $S$ is not empty\footnote{while this is obvious, it's an important step to go through in order to be able to apply the least upper bound property} as, e.g., $a - 1 \in S$. So by the least upper bound property, $\sup S$ exists. Let $r = \sup S$. We are claiming that $r = a$. We prove this by contradiction. To do this, suppose $r \neq a$. Then either $r < a$ or $r > a$. For the first case, suppose $r < a$. Then we can find an element $s$ between $r$ and $a$ ($r < s < a$) in order to reach a contradiction. Let $s = r + (a - r)/2$ (the midpoint between $r$ and $a$). Then $0 \leq s < a$, so $s \in S$. But also $s > r$ (as $a - r > 0$ so $s = r + (a - r)/2 > r$), so $r$ would not actually be an upper bound (since it is not greater than or equal to $s$), a contradiction. Now suppose $r > a$. But $a$ is an upper bound for $S$ (as, for all $r \in S$, $r < a$, so $a$ is an upper bound of $S$) with $a < r$, contradicting the minimality of $r$. So we must have that $r = a$.\footnote{Apologies for the verbosity, but hopefully this makes the proof easier to follow.}
\end{example}

The least upper bound property does not apply to, e.g., $\mathbb{Q}$. Any subset of $\mathbb{Q}$ whose least upper bound is irrational is an example of this, such as $\{r \in \mathbb{Q} : 0 \leq r < \sqrt{2}\}$ (since $\sqrt{2} \notin \mathbb{Q}$).

The least upper bound property can be used to prove the existence of square roots. Taking the minimum element of a set is a standard technique for proving the division algorithm for integers. 

\begin{proposition}
Say $\sup S \in S$. Then $\sup S \ge s$ $\forall s \in S$, so by definition $\sup S = \max S$.
\end{proposition}

\begin{proposition} \label{bound-comparison}
Let $S$ be bounded and nonempty. Then $\inf S$ and $\sup S$ exist. Let $s \in S$. Then $\inf S \le s \le \sup S$.
So $\inf S \le \sup S$.
\end{proposition}

\begin{proposition}
In Proposition \ref{bound-comparison}, suppose $\inf S = \sup S$. Then $\inf S = s = \sup S$, so $S = \{s\}$.
\end{proposition}

\subsection{Sequences, series, and limits}

\begin{definition} 
A sequence of real numbers $a_1$, $a_2$, $\ldots$ \textbf{converges} to $a \in \mathbb{R}$ if for all $\epsilon > 0$ there exists $N \in \mathbb{N}$ such that for all $n > N$,

\begin{center}
$|a_n - a| < \epsilon$
\end{center}

If this is the case, the sequence is called \textbf{convergent} and we can write $\{a_n\} \to a$ or $\lim_{n \to \infty} a_n = a$. $a$ is the \textbf{limit} of $\{a_n\}$. If it does not converge, it \textbf{diverges} (is divergent).
\end{definition}

This means we can take $\{a_n\}$ as close as we want to $a$ by finding an $N$ for any given $\epsilon$ such that $|a_n - a| < \epsilon$. We can take epsilon as small as we like as long as $\epsilon > 0$.

Actually proving that a sequence converges winds up being an exercise in grappling with sometimes tedious
absolute value expressions.

Convergent sequences compose the way you'd expect.

\pagebreak

\begin{proposition}
Let $\{a_n\}$ and $\{b_n\}$ be convergent sequences such that

\begin{align}
\lim_{n \to \infty} a_n = a, \lim_{n \to \infty} b_n = b.
\end{align}

Then

\begin{align}
\lim_{n \to \infty} (a_n + b_n) = a + b \\
\lim_{n \to \infty} (a_n b_n) = ab.
\end{align}

If $a \neq 0$ and $a_n \neq 0$ for all $n$,

\begin{equation}
\lim_{n \to \infty} \frac{1}{a_n} = \frac{1}{a}.
\end{equation}

If $b \neq 0$ and $b_n \neq 0$ for all $n$,

\begin{equation}
\lim_{n \to \infty} \frac{a_n}{b_n} = \frac{a}{b}.
\end{equation}

\end{proposition}

\begin{proof}
Let $\epsilon > 0$. There exists $N_1$ such that for all $n > N_1$, $|a_n - a| < \epsilon/2$, as we can take any difference we like as long as it's greater than zero, and $\epsilon/2 > 0$ since $\epsilon > 0$ and $2 > 0$. Likewise there exists $N_2$ such that for all $n > N_2$, $|b_n - b| < \epsilon/2$.

Take $N = \max(N_1, N_2)$. Then for all $n > N$, by rearranging terms and using the triangle inequality,

\begin{align}
|(a_n + b_n) - (a+b)| & =  |(a_n - a) + (b_n - b)| \\
& \leq  |a_n - a| + |b_n - b| \\
& <  \epsilon/2 + \epsilon/2 = \epsilon.
\end{align}

So $\lim_{n \to \infty} (a_n + b_n) = a + b$.

The other claims in the theorem have ugly algebra so I'm not going to type it here.
\end{proof}

\begin{theorem}[The squeeze theorem] Let $a \in \mathbb{R}$. Let $n, N \in \mathbb{N}$ and let $\{a_n\}$, $\{b_n\}$, and $\{c_n\}$ be sequences. Suppose for all $n > N$,

\begin{center}
$a_n \leq b_n \leq c_n $\\
$\lim_{n \to \infty} a_n = \lim_{n \to \infty} c_n = a$.
\end{center}

Then

\begin{equation}
\lim_{n \to \infty} b_n = a.
\end{equation}

\end{theorem}

\begin{definition}
A sequence $\{a_n\}$ is \textbf{increasing} if $a_n \leq a_{n+1}$ for all $n$. It is \textbf{decreasing} if $a_n \geq a_{n+1}$. $\{a_n\}$ is \textbf{monotonic} if it is increasing or decreasing.
\end{definition}

\begin{definition}
If a sequence $\{a_n\}$ is contained in an interval $[-B, B]$ such that $|a_n| <= B$, then $\{a_n\}$ is \textbf{bounded}. $B$ is a \textbf{bound}.

$\{a_n\}$ is \textbf{bounded above} if there exists some $K \in \mathbb{R}$ such that $a_n < K$ for all $n$. $\{a_n\}$ is \textbf{bounded below} if there exists $M \in \mathbb{R}$ such that $M < a_n$ for all $n$.
\end{definition}

\begin{theorem} \label{convergent-bounded}
Every convergent sequence is bounded.
\end{theorem}

\begin{proof}
Let $\{a_n\}$ be a convergence sequence. Let $\epsilon > 0$. By assumption, there exists $N$ such that for all $n > N$, $|a_n - a| < \epsilon$. In particular, when $\epsilon = 1$, $|a_n - a| < 1$. For all $n > N$, 

\begin{align}
|a_n| & = |a + (a_n - a)| \\
& \leq |a| + |a_n - \epsilon| \\
& < |a| + 1.
\end{align}

Let $\alpha = \max(|a_1|, |a_2|, \ldots, |a_N|, |a| + 1)$. By construction, $\alpha \geq |a_i|$ for $1 \leq i \leq N$. For $i > N$, $|a_i| < |a| + 1 \leq \alpha$. So $|a_i| \leq \alpha$ for all $i$. So by definition of boundedness, $\{a_n\}$ is bounded, so we are done.
\end{proof}

The contrapositive of Theorem \ref{convergent-bounded} is that if a sequence is not bounded, it is divergent. So it suffices to show that a series is not bounded to show it diverges.

\begin{theorem}[The monotone convergence theorem]
Every bounded monotone sequence converges.
\end{theorem}

So to show that a sequence converges, it suffices to show boundedness and increasing or decreasing.

\begin{definition}
Fix some $r \in \mathbb{R}$. Let $\{a_n\}$ be a sequence such that $a_{n+1} = ra_n$. Then $\{a_n\}$ is a \textbf{geometric sequence}.
\end{definition}

\begin{theorem}
Consider the sequence $\{r^n\}$, a geometric sequence whose terms have ratio $r$. 

\begin{itemize}
\item If $|r| < 1$, $\{r^n\} \to 0$.
\item If $r = 1$, $\{1^n\} \to 1$.
\item If $r > 1$ or $r \leq -1$, $\{r^n\}$ diverges.
\end{itemize}

\end{theorem}

\begin{definition}
Given a sequence $\{a_n\}$, define a \textbf{series} $\{s_n\} = \sum\limits_{i=1}^n a_i$.

If the limit $\sum\limits_{i=1}^\infty a_i = \lim_{n \to \infty} s_n$ exists, $\{s_n\}$ converges. Otherwise, it diverges.
\end{definition}

\begin{theorem} \label{convergent-series-sequence}
If $\sum\limits_{i=1}^\infty a_i = \lim_{n \to \infty} s_n$ converges, $\lim_{n \to \infty} a_n = 0$.
\end{theorem}

Taking the contrapositive of Theorem \ref{convergent-series-sequence} indicates that if $\lim_{n \to \infty} a_n \neq 0$, the associated series diverges.

\begin{example} 
The most common example of a sequence that converges but whose series diverges is $\{\frac{1}{n}\}$. $\lim_{n \to \infty} \frac{1}{n} = 0$, but the sum of its terms is unbounded, so the series $\sum\limits_{n=1}^\infty \frac{1}{n}$ diverges. So merely a summation expression whose individual values approach zero does not suffice for convergence. And actually the difference of terms in the series approach zero, which shows that, too, is insufficient evidence for convergence. This is perhaps a simple example of where something that may seem to be ``intuitively true" or ``obvious" is in fact false, one of many when dealing with infinities and other things the human brain is not good at intuitively reasoning about.
\end{example}

\begin{theorem} \label{geometric-series}
Let $|r| < 1$. The geometric series $s_n = 1 + r + r^2 + \ldots + r^n$ converges:

\begin{equation}
\sum\limits_{n=0}^\infty r^n = \frac{1}{1-r}
\end{equation}

and $s_n = \frac{1 - r^{n+1}}{1 - r}$. If $|r| >= 1$, the series diverges.
\end{theorem}

\begin{theorem}[Comparison theorem]
Let $\{a_n\}$ and $\{b_n\}$ be sequences such that for all $n$,

\begin{center}
$0 \leq a_n \leq b_n$.
\end{center}

If $\sum\limits_{n=1}^\infty b_n$ converges, then $\sum\limits_{n=0}^\infty a_n$ converges.
\end{theorem}

\begin{example}
We can use the comparison theorem to show that $\sum\limits_{n=1}^\infty \frac{1}{n^2}$ converges. As $n > 0$, we have that:

\begin{equation}\label{comparison-inequality}
\frac{1}{n^2} < \frac{1}{n(n-1)}
\end{equation}

as $n^2 > n^2 -n = n(n-1)$, so inverting it produces \ref{comparison-inequality}. Proceeding, we have

\begin{equation}
\frac{1}{n(n-1)} = \frac{n - (n-1)}{n(n-1)} = \frac{1}{n-1} - \frac{1}{n},
\end{equation}

so 

\begin{align}
\sum\limits_{n=2}^k \frac{1}{n(n-1)} & = \sum\limits_{n=2}^k \left(\frac{1}{n-1} - \frac{1}{n}\right) \\
& = 1 - \frac{1}{k}.
\end{align}

So $\sum\limits_{n=1}^k \frac{1}{n^2} < 1 - 1/k$, and $\lim_{k \to \infty} (1 - 1/k) = 1 - 0 = 1$, so it converges. So our series is less than another sequence we know converges, so by the comparison theorem, $\sum\limits_{n=1}^k \frac{1}{n^2}$ converges.
\end{example}

\begin{theorem}[Limit comparison theorem]
Let $\sum\limits_{n=1}^\infty a_n$ and $\sum\limits_{n=1}^\infty a_n$ be series of positive terms. If $\lim_{n \to \infty} \frac{a_n}{b_n}$ exists and is positive, $\sum\limits_{n=1}^\infty a_n$ converges if and only if $\sum\limits_{n=1}^\infty b_n$ converges.
\end{theorem}

\begin{theorem} \label{absolute-convergence-theorem}
If $\sum\limits_{n=1}^\infty |a_n|$ converges, so does $\sum\limits_{n=1}^\infty a_n$.
\end{theorem}

\begin{theorem}[Alternating series theorem]
Let $\{a_n\}$ be a decreasing non-negative ($ a_1 \geq a_2 \geq \ldots \geq 0$) sequence with $\lim_{n \to \infty} a_n = 0$. Then $\sum\limits_{n=1}^\infty (-1)^n a_n$ converges.
\end{theorem}

\begin{definition}
If  $\sum\limits_{n=1}^\infty a_n$ converges and  $\sum\limits_{n=1}^\infty |a_n|$ diverges,  $\sum\limits_{n=1}^\infty a_n$ converges \textbf{conditionally}. If  $\sum\limits_{n=1}^\infty |a_n|$ converges (and therefore  $\sum\limits_{n=1}^\infty a_n$ converges by Theorem \ref{absolute-convergence-theorem}, so both converge), $\sum\limits_{n=1}^\infty a_n$ converges \textbf{absolutely}.
\end{definition}

\begin{theorem}[Ratio test]
Let $\{a_n\}$ be a sequence with $\lim_{n \to \infty} |\frac{a_{n+1}}{a_n}| = L \in \mathbb{R}$.

\begin{itemize}
\item If $L < 1$, $\sum\limits_{n=1}^\infty a_n$ converges absolutely.
\item If $L > 1$, $\sum\limits_{n=1}^\infty a_n$ diverges.
\item The case $L = 1$ gives no information.
\end{itemize}
\end{theorem}

\begin{example}
Consider the sequence $\{\frac{5^n}{7^n}\}$. $\lim_{n \to \infty} |\frac{a_{n+1}}{a_n}| = \frac{5}{7} < 1$, so by the ratio test, the series $\sum\limits_{n=1}^\infty \frac{5^n}{7^n}$ converges absolutely. By Theorem \ref{convergent-series-sequence}, the sequence converges to 0. By Theorem \ref{geometric-series}, the limit of the series is $\frac{1}{1-5/7}$ where $r = 5/7$.
\end{example}

\begin{theorem}[Nested interval theorem]
Let $\{I_n\}$ be a sequence of closed intervals such that each element of the sequence is nested in the previous one (i.e., $I_{n+1} \subset I_n$ for all $n$), each term of $\{I_n\}$ has at least one point in common. If the length of $I_n$ approaches 0, there is exactly one point in common.
\end{theorem}

\begin{definition}[Cauchy criterion]
A real sequence $\{a_n\}$ is a \textbf{Cauchy sequence} if for all $\epsilon > 0$, there exists $N \in \mathbb{N}$ such that for all $n, m > N$, 

\begin{equation}
|a_n - a_m| < \epsilon.
\end{equation}
\end{definition}

\begin{theorem}
A sequence is Cauchy if and only if it converges.
\end{theorem}

\subsection{Functions and their limits}

\begin{definition}[$(\epsilon, \delta)$ limit of a function]

Let $f: \mathbb{R} \to \mathbb{R}$ be a function. 

\begin{center}
$\lim_{x \to a} f(x) = L$
\end{center}

if for every $\epsilon > 0$ there exists $\delta > 0$ such that if $0 < |x-a| <
\delta$, then $|f(x) - L| < \epsilon$.
 
\end{definition}

Analogous to the definition of a convergent sequence, this formalizes the idea of being able to take $f(x)$ as close to $L$ as we want
by making $x$ sufficiently close to $a$.

\begin{example} 

\emph{Show that $\lim_{x \to 1} (3x-1) = 2$}.

Substituting $f(x) = 3x-1$, $a = 1$, and $L = 2$ into the definition of a limit,
we want to show that

\begin{equation}
|x - 1| < \delta \implies |3x - 3| < \epsilon.
\end{equation}

This example was clearly chosen because the algebra works out nicely. It
works out like so: $|3x-3| = |3(x-1)| = 3|x-1| < \epsilon \implies |x-1| <
\epsilon/3$, so $\delta = \epsilon/3$ is a suitable choice.

Then $|x - 1| < \epsilon/3 \implies |3x-3| = |(x - 1) + (2x-2)| \leq |x-1| +
|2x-2| = |x-1| + 2|x-1| < \epsilon/3 + 2 \cdot \epsilon/3 = \epsilon,$ so we are
done.

Note that we have used Eq. \ref{triangle-inequality} above.
 
\end{example}

As you can no doubt surmise, with harder examples this shit gets really depressing.

Limits compose the way you'd expect:
\begin{proposition}
Suppose $\lim_{x \to a} f(x) = L$ and $\lim_{x \to a} g(x) = M$. Then

\begin{align}
\lim_{x \to a} f(x) + g(x) & = L + M \\
\lim_{x \to a} f(x)g(x) & = L \cdot M \\
\lim_{x \to a} \left(\frac{f}{g}\right)(x) & = \frac{L}{M}
\end{align}

Assuming everything above is defined.

\end{proposition}

If you can show that the positive ($\lim_{x \to a^{+}} f(x)$) and negative
($\lim_{x \to a^{-}} f(x) $) limits of a function are
different, it follows that the limit does not exist.


\subsection{Continuity}

\begin{definition}

$f$ is \textbf{continuous} at $a$ if

\begin{equation}
\lim_{x \to a} f(x) = f(a).
\end{equation}

\end{definition}

Of course continuous functions compose as you'd expect.

\begin{proposition}

If $f$ and $g$ are continuous at $a$, then $f+g$ and $f \cdot g$ are continuous
at $a$. If $g(a) \neq 0$, $1/g$ is continuous at $a$.

\end{proposition}

\begin{proof}

This follows from the definition of continuity and the previous limit
theorem. $\lim_{x \to a} (f(x) + g(x)) = \lim_{x \to a} f(x) + \lim_{x \to a}
g(x) = f(a) + g(a)$, so $f + g$ is continuous at $a$ by definition of
continuity. Replacing $+$ with $\cdot$ and using the previous limit theorem, the
rest of the result follows.

\end{proof}

The key thing that happens with a continuous function is that there's an
interval around $a$ that will share the properties of $f(a)$, like sign.

\subsection{Derivatives}

\begin{definition} 
$f$ is \textbf{differentiable} at $a$ if

\begin{equation}
\lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\end{equation}

exists.
\end{definition}

Recall the geometric interpretation of $f'(x)$ as being the slope of a line
tangent to $f(x)$; $f'(x)$ is like a secant line on a function between two
points of infinitely small distance. The physical interpretation of $f'(x)$ as
velocity and $f''(x)$ as acceleration should be familiar.

Every differentiable function is continuous, but not every continuous function is differentiable. The standard example of this is $f(x) = |x|$.

\begin{proposition}[Product Rule] 

If $f$ and $g$ are differentiable, 

\begin{equation}
(f \cdot g)' = f'g + g'f.
\end{equation}


\end{proposition} 

This generalizes recursively; $(f \cdot g \cdot h)' = f'gh + fg'h + fgh'$, and
so on. Notice the symbolic symmetry of it: sum the product of all the functions,
differentiating each one in turn.

\begin{proposition}[Chain Rule]

If $f$ and $g$ are differentiable,

\begin{equation}
(f \circ g)'(x) = f'(g)g'(x).
\end{equation}


\end{proposition}

\subsubsection{Derivative formulas}

\begin{equation}
\left(\frac{f}{g}\right)' = \frac{f'g - g'f}{g^2}
\end{equation}

\begin{equation}
(x^n)' = nx^{n-1}
\end{equation}

\section{Books I copied this stuff from} \label{bibliography}

\begin{thebibliography}{9}

\bibitem{ross}
  Kenneth A. Ross,
  \emph{Elementary Analysis: The Theory of Calculus}.
  Springer-Verlag,
  1st Edition,
  2013.

\bibitem{smith}
  Geoff Smith,
  \emph{Introductory Mathematics: Algebra and Analysis},
  Springer-Verlag,
  1998.

\bibitem{spivak}
  Michael Spivak,
  \emph{Calculus}.
  Publish or Perish,
  4th Edition,
  2008.

\bibitem{strichartz}
  Robert Strichartz,
  \emph{The Way of Analysis}.
  Jones and Bartlett Publishers,
  2000.
  
\end{thebibliography}

\section{License}

This document is freely shareable under the terms of the GNU Free Documentation
license; see LICENSE in this document's directory for more information.

\end{document}

